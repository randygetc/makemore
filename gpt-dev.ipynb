{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51cd33d8-9462-4414-b4d3-51eefc610b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3446e92f-4d7f-4206-82cc-d8cff317aaff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset:  1115394\n"
     ]
    }
   ],
   "source": [
    "print('Size of dataset: ', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "495c7137-675d-4823-b2e3-a4e99cbcb4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print first 1000 charactrs\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4394d74c-078f-41a3-969c-e8ddbf4776e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the unique characters from the text file set(text)\n",
    "# convert to a list list(set(txt))\n",
    "# sort\n",
    "chars = sorted(list(set(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdfe62fa-1ee6-42cb-b4df-416ef0de68b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82d8cd0f-d6de-44a2-a179-9a4af02f10e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 43, 50, 50, 53, 1, 58, 46, 43, 56, 43]\n",
      "hello there\n"
     ]
    }
   ],
   "source": [
    "# tokenize\n",
    "# enumerate` returns an iterator that produces tuples containing a \n",
    "# count (from start which defaults to 0) and the values obtained from iterating over the sequence (`chars`).\n",
    "stoi = { ch:i for i,ch in enumerate(chars)}\n",
    "itos = { i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "# encode, decode\n",
    "# The `encode` function takes a string `s` and returns a list of integers. For each character `c` in the string, \n",
    "# ot looks up its corresponding index in the `stoi` dictionary and adds it to the list.\n",
    "encode = lambda s: [stoi[c] for c in s]  \n",
    "\n",
    "# The `decode` function takes a list of integers `l` and returns a string. For each index `i` in the list, \n",
    "#it looks up its corresponding character in the `itos` dictionary and joins all characters together using the `''.join()` method\n",
    "\n",
    "decode = lambda l: ''.join(itos[i] for i in l)     # decoderL take a list of integers, convert to string\n",
    "\n",
    "print(encode('hello there'))\n",
    "print(decode(encode('hello there')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14721d2a-875e-40fb-a07f-084057bdc971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50257\n",
      "[71, 4178, 612]\n"
     ]
    }
   ],
   "source": [
    "# tokenizer\n",
    "# google uses sentencepiece - subword unit\n",
    "# opeani uses tiktoken\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "print(enc.n_vocab)               # print size of vocabulary\n",
    "print(enc.encode('hii there'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0eab2d7f-47ee-4ca8-85e0-745cdac4d8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "# encode entire text dataset and store it into a torch.tensor\n",
    "import torch\n",
    "data = torch.tensor(encode(text), dtype = torch.long)\n",
    "print(data.shape, data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4cc930-2545-4ebd-9718-d190e01ae345",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64e18f9f-d29b-4c65-8cf5-21159e15020d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split up data into train and validation sets\n",
    "n = int(0.8*len(data))\n",
    "train_data = data[0:n]\n",
    "val_data    = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f9322f5-8e86-4c8e-aba0-fbad69c05bbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "45fd8237-6ba3-4e51-a524-3b88add5c934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is: tensor([18]), the target is: 47\n",
      "when input is: tensor([18, 47]), the target is: 56\n",
      "when input is: tensor([18, 47, 56]), the target is: 57\n",
      "when input is: tensor([18, 47, 56, 57]), the target is: 58\n",
      "when input is: tensor([18, 47, 56, 57, 58]), the target is: 1\n",
      "when input is: tensor([18, 47, 56, 57, 58,  1]), the target is: 15\n",
      "when input is: tensor([18, 47, 56, 57, 58,  1, 15]), the target is: 47\n",
      "when input is: tensor([18, 47, 56, 57, 58,  1, 15, 47]), the target is: 58\n"
     ]
    }
   ],
   "source": [
    "# target is the next char give a char or a string of char\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is: {context}, the target is: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b77d7c10-ccae-4d73-b45e-eaf07985ef76",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "batch_size = 4           # how many indepenent sequences will be processed in parallel\n",
    "block_size = 8           # what is the max context length for predictions\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x,y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "52449be9-0adc-4f7e-bf76-ed9f4e26b13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[58, 63,  8,  0,  0, 19, 24, 27],\n",
      "        [39, 59, 45, 46, 58,  1, 46, 43],\n",
      "        [49, 43, 57,  1, 53, 50, 42,  1],\n",
      "        [52, 41, 47, 43, 52, 58,  1, 56]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[63,  8,  0,  0, 19, 24, 27, 33],\n",
      "        [59, 45, 46, 58,  1, 46, 43,  1],\n",
      "        [43, 57,  1, 53, 50, 42,  1, 46],\n",
      "        [41, 47, 43, 52, 58,  1, 56, 47]])\n"
     ]
    }
   ],
   "source": [
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1311cf2-0cf9-48f9-bb0d-a558dbc7ced3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d8c81625-efd4-4bc4-99c2-a81ecdfd6116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "when input is: [58], the target is: 63\n",
      "when input is: [58, 63], the target is: 8\n",
      "when input is: [58, 63, 8], the target is: 0\n",
      "when input is: [58, 63, 8, 0], the target is: 0\n",
      "when input is: [58, 63, 8, 0, 0], the target is: 19\n",
      "when input is: [58, 63, 8, 0, 0, 19], the target is: 24\n",
      "when input is: [58, 63, 8, 0, 0, 19, 24], the target is: 27\n",
      "when input is: [58, 63, 8, 0, 0, 19, 24, 27], the target is: 33\n",
      "when input is: [39], the target is: 59\n",
      "when input is: [39, 59], the target is: 45\n",
      "when input is: [39, 59, 45], the target is: 46\n",
      "when input is: [39, 59, 45, 46], the target is: 58\n",
      "when input is: [39, 59, 45, 46, 58], the target is: 1\n",
      "when input is: [39, 59, 45, 46, 58, 1], the target is: 46\n",
      "when input is: [39, 59, 45, 46, 58, 1, 46], the target is: 43\n",
      "when input is: [39, 59, 45, 46, 58, 1, 46, 43], the target is: 1\n",
      "when input is: [49], the target is: 43\n",
      "when input is: [49, 43], the target is: 57\n",
      "when input is: [49, 43, 57], the target is: 1\n",
      "when input is: [49, 43, 57, 1], the target is: 53\n",
      "when input is: [49, 43, 57, 1, 53], the target is: 50\n",
      "when input is: [49, 43, 57, 1, 53, 50], the target is: 42\n",
      "when input is: [49, 43, 57, 1, 53, 50, 42], the target is: 1\n",
      "when input is: [49, 43, 57, 1, 53, 50, 42, 1], the target is: 46\n",
      "when input is: [52], the target is: 41\n",
      "when input is: [52, 41], the target is: 47\n",
      "when input is: [52, 41, 47], the target is: 43\n",
      "when input is: [52, 41, 47, 43], the target is: 52\n",
      "when input is: [52, 41, 47, 43, 52], the target is: 58\n",
      "when input is: [52, 41, 47, 43, 52, 58], the target is: 1\n",
      "when input is: [52, 41, 47, 43, 52, 58, 1], the target is: 56\n",
      "when input is: [52, 41, 47, 43, 52, 58, 1, 56], the target is: 47\n"
     ]
    }
   ],
   "source": [
    "print(\"------\")\n",
    "for b in range(batch_size):          # batch size\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is: {context.tolist()}, the target is: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a2872b64-efc1-43de-833a-0fd156e8f20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# This line defines a new class called `BigramLanguageModel`, which inherits from PyTorch's `nn.Module` class.\n",
    "#  It uses a lookup table (token embedding table) to map each token to a numerical vector representation. \n",
    "# The model can be trained on a dataset of text data to learn the patterns and relationships between tokens.\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        # The `super().__init__()` line calls the `__init__` method of the parent class (`nn.Module`).\n",
    "        super().__init__()\n",
    "        # each token directly reads of the logits for th next token from the lookup table\n",
    "\n",
    "        # The `self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)` line creates an embedding table for the tokens. \n",
    "        # An embedding table is a lookup table that maps integer indices to dense vectors. In this case, the table has \n",
    "        # `vocab_size` rows (one for each token in the vocabulary) and `vocab_size` columns \n",
    "        # (since we're using a single vector to represent each token).\n",
    "        \n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "\n",
    "    # The `forward` method is a special method in PyTorch modules that defines the forward pass through the network. \n",
    "    # In this case, it takes two arguments `idx` and `targets`, both of which are tensors of integers.\n",
    "    def forward(self, idx, targets):\n",
    "\n",
    "        # ifx and targets are both (B,T) tensor of integers\n",
    "        # The `logits = self.token_embedding_table(idx)` line looks up the embeddings for the tokens in the input tensor `idx`. \n",
    "        # The resulting tensor has shape `(B,T,C)`, where `B` is the batch size, \n",
    "        #`T` is the sequence length, and `C` is the embedding size (which is `vocab_size` in this case).\n",
    "\n",
    "        logits = self.token_embedding_table(idx)   # (B,T,C)\n",
    "        \n",
    "        B, T, C = logits.shape\n",
    "        logits = logits.view(B*T, C)\n",
    "        targets = targets.view(B*T)\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "      \n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx)\n",
    "            logits = logits[:, -1, :]\n",
    "            # apply softmax\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples =1)\n",
    "            # append sample index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1a8494db-0ffe-4700-aaa2-b95535ca9f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.5553, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "m = BigramLanguageModel(vocab_size)\n",
    "logits,loss = m(xb,yb)\n",
    "print(logits.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb4ca3e-5ecf-4e9f-b379-c1d411d2b364",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
